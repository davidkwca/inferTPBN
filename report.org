#+TITLE:     Report: Probabilistic Boolean Networks
#+AUTHOR:    David Kaltenpoth
#+DATE:      \today
#+LANGUAGE:  en
#+LATEX_HEADER: \input{/home/dk/Dropbox/preamble.tex}

#+STARTUP: oddeven

#+OPTIONS:   H:2 toc:nil

# the following allow us to selectively choose headlines to export or not
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
* Introduction
  With the ubiquitous availability of *omics data, and the shortcoming of classical methods in dealing with these sources, systems biology has come to the forefront of biological inquiry.
  Different modelling approaches try to infer the cellular networks underlying the data generating process in a variety of ways.

  One of these methods is logical modelling, where the state of each gene \(g\) takes values in \(\left\{ 0,1 \right\}\), and is regulated by a number of other genes \(g_{1},\dots,g_m\).
* Probabilistic Boolean Networks
  For Boolean Networks (BNs) cite:kauffman1993origins, we consider \(n\) genes \(g_1,\dots,g_n \in \left\{ 0,1 \right\}\) and logical functions \(f_{i} : \left\{ 0,1 \right\}^n \rightarrow \left\{ 0, 1 \right\} \) so that
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i(g_1(t),...,g_m(t)).
  \end{align*}
  #+END_EXPORT
  The set \(G_i\) of genes on which \(f_i\) is not constant are the /regulators/ of \(g_i\). Hence, we can also understand \(f_i\) as a function on \(\left\{ 0,1 \right\}^{\left| G_i \right|}\).

  Since cells are noisy, it makes sense to allow for perturbations. These models are called Boolean Networks with perturbations (BNps). In this case
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i(g_1(t),...,g_m(t)) \oplus z_i
  \end{align*}
  #+END_EXPORT
  where the \(z_i \sim \text{Bern}(p)\) are i.i.d.

  To arrive at Probabilistic Boolean Networks (PBNs), we allow each gene \(g_i\) to be regulated by a set of functions, \(\left\{ f_i^{(l)} : l = 1,...,m_i \right\}\), each picked with probability \(c_i^{(l)}\) such that
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i^{(l)}\left(g_1(t),...,g_m(t)\right) \oplus z_i \text{ with probability } c_i^{(l)}.
  \end{align*}
  #+END_EXPORT
** Threshold PBNs
   Since in general there are way too many Boolean functions \(f_i\) on \(m\) inputs and therefore the model class is too rich, we restrict ourselves to a more restricted set of PBNs, called /Threshold PBNs/. Here, the Boolean functions \(f_i^{(l)}\) are of the form
   #+BEGIN_EXPORT latex
   \begin{align*}
   f_i^{(l)}\left( g_1,...,g_m\right) = \left\{
     \begin{array}{cc}
       1, & \sum_{j = 1}^ma_{ij}^{(l)}g_j > 0 \\
       0, & \sum_{j = 1}^ma_{ij}^{(l)}g_j < 0 \\
       g_i, & \text{ otherwise}
     \end{array}\right.
   \end{align*}
   #+END_EXPORT
* Method
** The loss function
   # TODO: not 2^2^k, but instead more like 2^k. Also think about the entropy term again.
   Given a timeseries of gene measurements \(D = \left\{ g_i(t) : t = 1,...,T, i = 1,...,n \right\}\), we can look at the probability of \(D\) under a given PBN \(N\):
   #+BEGIN_EXPORT latex
   \begin{align*}
     P(D | N) &= \prod_{t = 2}^{T} \prod_{i = 1}^m P\left(g_i(t) | G(t-1), N\right) \\
              &= \prod_{t = 2}^{T} \prod_{i = 1}^m \left( p \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(G(t-1)) = g_i(t)} + (1-p) \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(G(t-1)) \neq g_i(t)} \right),
   \end{align*}
   #+END_EXPORT
   where \(G(t-1) = \left\{ g_i(t-1) : i = 1,...,n \right\}\). Here we used that the process generated is a first order Markov chain and that \(G(t)\) is independent given \(G(t-1)\).

   Furthermore, for the prior \(P(N)\) we use an MDL approach as follows.

   Generally, we encode each \(f_i^{(l)}\) independently, i.e. we do not assume that different functions share any input or structure worth exploiting.

   Hence, we assume that \(-\log P(N)\) is of the form
   #+BEGIN_EXPORT latex
   \begin{align*}
   -\log P(N) = \sum_{i, l}^{} L(f_i^{(l)})
   \end{align*}
   #+END_EXPORT
   where \(L(f)\) is an encoding of the function \(f\).

   In the following, all logarithms are taken to the base two.

   First, for each function \(f_{i}^{(l)}\) we have to encode how many inputs \(k_{i,l}\) it takes, taking \(L_{\mathbb{N}}(k_{i,l})\) bits, where \(L_{\mathbb{N}}(n) = \log n + \log\log n + \log\log\log n + ... + \log c_0 \) where sum goes only over positive terms, and \(c_0 \approx 2.865\) is such that \(\sum_n^{}2^{-L_{\mathbb{N}}(n)} = 1\).

   We also need to encode which inputs it takes which we can do using \(\log \binom{n}{k_{i,l}}\) bits.

   Furthermore, at we need to encode the parameters \(a_i \in \left\{ -1, 1 \right\}\) which requires \(\log 2^{k_{i,l}}\) bits.

   The overall prior for \(N\) is therefore
   #+BEGIN_EXPORT latex
   \begin{align*}
   -\log P(N) = \sum_{i = 1}^m \left( \sum_{l = 1}^{m_i} L_{\mathbb{N}}(k_{i,l}) + \log \binom{n}{k_{i,l}} + \log 2^{k_{i,l}} \right)
   \end{align*}
   #+END_EXPORT

   We therefore need to minimize the overall loss function
   #+BEGIN_EXPORT latex
   \begin{align*}
     &-\log P(D | N) - \log P(N) \\
     = &\prod_{t = 2}^{T} \prod_{i = 1}^m \left( p \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) = g_i(t)} + (1-p) \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) \neq g_i(t)} \right)\\
     &+ \sum_{i = 1}^m \left( \sum_{l = 1}^{m_i} L_{\mathbb{N}}(k_{i,l}) \log \binom{n}{k_{i,l}} + \log 2^{k_{i,l}} \right)
   \end{align*}
   #+END_EXPORT
   with respect to \(\left\{ a_j^{(l)}, c_j^{(l)} \right\}\).
** Minimizing the Loss
   It shouldn't be surprising that global optimization of this loss function is not feasible.
   We therefore use a greedy bottom-up approach to find a PBN which explains the data well.

   Further, since the optimizations for every gene are independent, we will focus only on the optimization for \(g = g_1\) across all time steps, and drop all unnecessary indices.

   Starting from the empty net \(N_0\), we start by building a single BN as follows:
   Given distinct \(G_k := g_{j_1},...,g_{j_k}\), \(A_k := a_{j_1},...,a_{j_k}\), find
   #+BEGIN_EXPORT latex
   \begin{align*}
   g_{j_{k+1}}, A_{k+1} := \argmin_{g', A'} \left(  -\log P\left(g | f_{(G_k, g'), A'}\right) + L(f_{(G_k, g'), A'}) \right),
   \end{align*}
   #+END_EXPORT
   where \(f_{G, A}\) is the threshold function with inputs \(G\) and parameters \(A\). The term \(-\log P(g | f_{G, A})\) should be understood as \(-\sum_{t = 2}^T\log P(g(t) | f_{G, A}, G(t-1)) \).

   That is, at every step we add a single gene to the regulatory set, and do a joint optimization over all parameters again.
   This joint optimization has been found to perform much better than picking only a single new \(a_{j_{k+1}}\) while leaving \(A_k\) fixed.

   New genes \(g'\) are added until the loss function \(-\log P(g | f) + L(f)\) doesn't improve any longer by adding more genes to the inputs of \(f\). In practice, limiting the number of potential inputs to \(k = 3, 5\) tends to be a good idea to improve running time without hurting performance too much.

   Once we have found \(f^{(1)},...,f^{(l)}\), we try to find a new \(f^{(l+1)}\) as follows.

   For lack of a better heuristic, start by setting \(c^{(l)}, c^{(l+1)} \leftarrow c^{(l)}/2\).
   Given \(c\), \(g^{(l+1)}_{j_1},...,g^{(l+1)}_{j_k}\), find \(g^{(l+1)}_{j_{k+1}}\) and \(A^{(l)}_{k+1}\) as before
   #+BEGIN_EXPORT latex
   \begin{align*}
     g_{j_{k+1}}, A_{k+1} := \argmin_{g', A'} \left(  -\log P\left(g | \left\{ f^{(i)},  f^{(l+1)}_{(G^{(l+1)}_k, g'), A'} \right\}\right) -\log P\left(\left\{ f^{(i)}, f^{(l+1)}_{(G^{(l+1)}_k, g'), A'} \right\}\right) \right),
   \end{align*}
   #+END_EXPORT
   i.e. we optimize the inputs and parameters for the new function given all other functions in the same way as we did when there were no other functions.

   Then, given all the \(\left\{ G_k^{(l)}, A_k^{(l)} \right\}\), we find
   #+BEGIN_EXPORT latex
   \begin{align*}
   c := \argmin_{c'} \left( - \log P(g | \left\{ f^{(i)} \right\}, c') - \log P(\left\{ f^{(i)} \right\}, c') \right),
   \end{align*}
   #+END_EXPORT
   i.e. we find the best probabilities given all the other parameters.

   The procedure stops when no new genes are found.

   After all \(f^{(i)}, c^{(i)}\) have been computed, we can prune all functions for which \(c^{(i)} < t\) where \(t\) is an arbitrary threshold. If no pruning is desired, \(t = 0\).

   A high-level overview of the algorithm is given in...
* Results
* Further Ideas
  From a coding perspective, instead of computing \(P(D | N)\) by summing over probabilities of networks predicting the correct outcome, one could also use \(H(\left\{ c_j^{(l)} : l = 1,...,L \right\})\) bits to encode which network is the "correct one" at every time step and try to find that model which requires the least number of bits to encode the outcome.
* Related research
  \printbibliography
