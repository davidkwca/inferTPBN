#+TITLE:     Report: Probabilistic Boolean Networks
#+AUTHOR:    David Kaltenpoth
#+DATE:      \today
#+LANGUAGE:  en
#+LATEX_HEADER: \input{/home/dk/Dropbox/preamble.tex}

#+STARTUP: oddeven

#+OPTIONS:   H:2 toc:nil

# the following allow us to selectively choose headlines to export or not
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
* Introduction
  With the ubiquitous availability of *omics data, and the shortcoming of classical methods in dealing with these sources, systems biology has come to the forefront of biological inquiry.
  Different modelling approaches try to infer the cellular networks underlying the data generating process in a variety of ways.

  One of these methods is logical modelling, where the state of each gene \(g\) takes values in \(\left\{ 0,1 \right\}\), and is regulated by a number of other genes \(g_{1},\dots,g_m\).
* Probabilistic Boolean Networks
  For Boolean Networks (BNs) cite:kauffman1993origins, we consider \(n\) genes \(g_1,\dots,g_n \in \left\{ 0,1 \right\}\) and logical functions \(f_{i} : \left\{ 0,1 \right\}^n \rightarrow \left\{ 0, 1 \right\} \) so that
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i(g_1(t),...,g_m(t)).
  \end{align*}
  #+END_EXPORT
  The set \(G_i\) of genes on which \(f_i\) is not constant are the /regulators/ of \(g_i\). Hence, we can also understand \(f_i\) as a function on \(\left\{ 0,1 \right\}^{\left| G_i \right|}\).

  Since cells are noisy, it makes sense to allow for perturbations. These models are called Boolean Networks with perturbations (BNps). In this case
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i(g_1(t),...,g_m(t)) \oplus z_i
  \end{align*}
  #+END_EXPORT
  where the \(z_i \sim \text{Bern}(p)\) are i.i.d.

  To arrive at Probabilistic Boolean Networks (PBNs), we allow each gene \(g_i\) to be regulated by a set of functions, \(\left\{ f_i^{(l)} : l = 1,...,m_i \right\}\), each picked with probability \(c_i^{(l)}\) such that
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i^{(l)}\left(g_1(t),...,g_m(t)\right) \oplus z_i \text{ with probability } c_i^{(l)}.
  \end{align*}
  #+END_EXPORT
** Threshold PBNs
   Since in general there are way too many Boolean functions \(f_i\) on \(m\) inputs and therefore the model class is too rich, we restrict ourselves to a more restricted set of PBNs, called /Threshold PBNs/. Here, the Boolean functions \(f_i^{(l)}\) are of the form
   #+BEGIN_EXPORT latex
   \begin{align*}
   f_i^{(l)}\left( g_1,...,g_m\right) = \left\{
     \begin{array}{cc}
       1, & \sum_{j = 1}^ma_{ij}^{(l)}g_j > 0 \\
       0, & \sum_{j = 1}^ma_{ij}^{(l)}g_j < 0 \\
       g_i, & \text{ otherwise}
     \end{array}\right.
   \end{align*}
   #+END_EXPORT
* The current method
** The loss function
   # TODO: not 2^2^k, but instead more like 2^k. Also think about the entropy term again.
   Given a timeseries of gene measurements \(D = \left\{ g_i(t) : t = 1,...,T \right\}\), we can look at the probability of \(D\) under a given PBN \(N\):
   #+BEGIN_EXPORT latex
   \begin{align*}
     P(D | N) &= \prod_{t = 2}^{T} \prod_{i = 1}^m P(g_i(t) | \left\{ g_j(t-1) \right\}, N) \\
              &= \prod_{t = 2}^{T} \prod_{i = 1}^m \left( p \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) = g_i(t)} + (1-p) \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) \neq g_i(t)} \right),
   \end{align*}
   #+END_EXPORT
   as the process generated by a PBN is a first order Markov chain and we assume the \(\left\{ g_i(t) \right\}\) to be independent given \(\left\{ g_i(t-1) \right\}\).

   Furthermore, for the prior \(P(N)\) we use an MDL approach as follows.

   First, each function \(f_{i}^{(l)}\) is encoded using \(\log \binom{n}{k_{i,l}} + \log 2^{2^{k_{i,l}}}\) bits, where \(k_{i,l}\) is the number of inputs on which \(f_{i}^{(l)}\) depends.

   Furthermore, to decide which network is being used for predicting \(g_i\) at each time point, we further require a penalty \(T \cdot H(\left\{ c_i^1,...,c_i^{(m_i)} \right\})\).

   The overall prior for \(N\) is therefore
   #+BEGIN_EXPORT latex
   \begin{align*}
   -\log P(N) = \sum_{i = 1}^m \left( \sum_{l = 1}^{m_i}\log \binom{n}{k_{i,l}} + \log 2^{2^{k_{i,l}}} \right) + T \cdot H(\left\{ c_i^1,...,c_i^{(m_i)} \right\})
   \end{align*}
   #+END_EXPORT

   We therefore need to minimize the overall loss function
   #+BEGIN_EXPORT latex
   \begin{align*}
     &-\log P(D | N) - \log P(N) \\
     = &\prod_{t = 2}^{T} \prod_{i = 1}^m \left( p \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) = g_i(t)} + (1-p) \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) \neq g_i(t)} \right)\\
     &+ \sum_{i = 1}^m \left( \sum_{l = 1}^{m_i}\log \binom{n}{k_{i,l}} + \log 2^{2^{k_{i,l}}} \right) + T \cdot H(\left\{ c_i^1,...,c_i^{(m_i)} \right\})
   \end{align*}
   #+END_EXPORT
   with respect to \(\left\{ a_j^{(l)}, c_j^{(l)} \right\}\).
** Optimization of the Loss Function
   It shouldn't be surprising that global optimization of this loss function is not feasible.
   We therefore use a greedy bottom-up approach to find a PBN which explains the data well.

   Further, since the optimizations for every gene are independent, we will focus only on the optimization for \(g = g_1\), and drop all unnecessary indices.

   Starting from the empty net \(N_0\), we start by building a single BN as follows:
   Given distinct \(G_k := g_{j_1},...,g_{j_k}\), \(A_k := a_{j_1},...,a_{j_k}\), find
   #+BEGIN_EXPORT latex
   \begin{align*}
   g_{j_{k+1}}, A_{k+1} := \argmin_{g', A'} \left(  -\log P(g | f_{(G_k, g'), A'}) -\log P(f_{(G_k, g'), A'}) \right),
   \end{align*}
   #+END_EXPORT
   that is at every step we add a single gene to the regulatory set, and do a joint optimization over all parameters again.
   This joint optimization has been found to perform much better than picking only a single new \(a_{j_{k+1}}\) while leaving \(A_k\) fixed.

   New genes \(g'\) are added until the loss function \(-\log P(g | f) - \log P(f)\) doesn't improve any longer by adding more genes to the regulatory inputs. In practice, limiting the number of potential inputs to \(k = 3, 5\) tends to be a good idea to improve running time without hurting performance too much.

   Once we have found \(f^{(1)},...,f^{(l)}\), we try to find a new \(f^{(l+1)}\) as follows.

   For lack of a better heuristic, start by setting \(c^{(l)}, c^{(l+1)} \leftarrow c^{(l)}/2\).
   Given \(c\), \(g^{(l+1)}_{j_1},...,g^{(l+1)}_{j_k}\), find \(g^{(l+1)}_{j_{k+1}}\) and \(A^{(l)}_{k+1}\) as before
   #+BEGIN_EXPORT latex
   \begin{align*}
     g_{j_{k+1}}, A_{k+1} := \argmin_{g', A'} \left(  -\log P(g | \left\{ f^{(i)},  f^{(l+1)}_{(G^{(l+1)}_k, g'), A'} \right\}) -\log P(\left\{ f^{(i)}, f^{(l+1)}_{(G^{(l+1)}_k, g'), A'} \right\}) \right),
   \end{align*}
   #+END_EXPORT
   i.e. we optimize the inputs and parameters for the new function given all other functions in the same way as we did when there were no other functions.

   Then, given all the \(\left\{ G_k^{(l)}, A_k^{(l)} \right\}\), we find
   #+BEGIN_EXPORT latex
   \begin{align*}
   c := \argmin_{c'} \left( - \log P(g | \left\{ f^{(i)} \right\}, c') - \log P(\left\{ f^{(i)} \right\}, c') \right),
   \end{align*}
   #+END_EXPORT
   i.e. we find the best probabilities given all the other parameters.

   The procedure stops when no new genes are found.

   After all \(f^{(i)}, c^{(i)}\) have been computed, we can prune all functions for which \(c^{(i)} < t\) where \(t\) is an arbitrary threshold. If no pruning is desired, \(t = 0\).

   A high-level overview of the algorithm is given in...
* Results
* Related research
  \printbibliography
