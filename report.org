#+TITLE:     Report: Probabilistic Boolean Networks
#+AUTHOR:    David Kaltenpoth
#+DATE:      \today
#+LANGUAGE:  en
#+LATEX_HEADER: \input{/home/dk/Dropbox/preamble.tex}
#+LATEX_HEADER: \usepackage{subfig}
#+LATEX_HEADER: \usepackage{float}

#+STARTUP: oddeven

#+OPTIONS:   H:2 toc:nil

# the following allow us to selectively choose headlines to export or not
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

* Introduction
  With the ubiquitous availability of *omics data, and the shortcoming of classical methods in dealing with these sources, systems biology has come to the forefront of biological inquiry.
  Within systems biology, different modelling approaches try to infer the cellular networks underlying the data generating process in a variety of ways.

  One of these methods is logical modelling, where the state of each gene \(g\) takes values in \(\left\{ 0,1 \right\}\), and is regulated by a number of other genes \(g_{1},\dots,g_m\).
* Probabilistic Boolean Networks
  For Boolean Networks (BNs) cite:kauffman1969metabolic,kauffman1993origins, we consider \(n\) genes \(g_1,\dots,g_n \in \left\{ 0,1 \right\}\) and logical functions \(f_{i} : \left\{ 0,1 \right\}^n \rightarrow \left\{ 0, 1 \right\} \) so that
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i(g_1(t),...,g_m(t)).
  \end{align*}
  #+END_EXPORT
  The set \(G_i\) of genes on which \(f_i\) is not constant are the /regulators/ of \(g_i\). Hence, we can also understand \(f_i\) as a function on \(\left\{ 0,1 \right\}^{\left| G_i \right|}\).

  Since cells are noisy, it makes sense to allow for perturbations. These models are called Boolean Networks with perturbations (BNps). In this case
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i(g_1(t),...,g_m(t)) \oplus z_i
  \end{align*}
  #+END_EXPORT
  where the \(z_i \sim \text{Bern}(p)\) are for convenience taken to be i.i.d. Of course, different noise levels could be used for different genes.

  To arrive at Probabilistic Boolean Networks (PBNs) cite:shmulevich2002probabilistic,shmulevich2010probabilistic, we allow each gene \(g_i\) to be regulated by a set of functions, \(\left\{ f_i^{(l)} : l = 1,...,m_i \right\}\), each picked with probability \(c_i^{(l)}\) such that
  #+BEGIN_EXPORT latex
  \begin{align*}
  g_i(t+1) = f_i^{(l)}\left(g_1(t),...,g_m(t)\right) \oplus z_i \text{ with probability } c_i^{(l)}.
  \end{align*}
  #+END_EXPORT
** Threshold PBNs
   Since in general there are far too many Boolean functions \(f_i\) on \(m\) inputs and therefore the model class is too rich, we restrict ourselves to a more restricted set of PBNs, called /Threshold PBNs/. Here, the Boolean functions \(f_i^{(l)}\) are of the form
   #+BEGIN_EXPORT latex
   \begin{align*}
   f_i^{(l)}\left( g_1,...,g_m\right) = \left\{
     \begin{array}{cc}
       1, & \sum_{j = 1}^ma_{ij}^{(l)}g_j > 0 \\
       0, & \sum_{j = 1}^ma_{ij}^{(l)}g_j < 0 \\
       g_i, & \text{ otherwise}
     \end{array}\right.
   \end{align*}
   #+END_EXPORT
   These restrictions are also quite plausible, and additive effects of regulators along with threshold dynamics are commonly used in systems biology cite:alon2006introduction,klipp2016systems.
* Method
** The loss function
   In the following, all logarithms are taken to the base two.

   Given a timeseries of gene measurements \(D = \left\{ g_i(t) : t = 1,...,T, i = 1,...,n \right\}\),
   our goal is to find that PBN \(N = \left\{ f_i^{(l)}, c_i^{(l)} \right\}\) which minimizes
   #+BEGIN_EXPORT latex
   \begin{align*}
   -\log P(N | D) = -\log P(D | N) - \log P(N).
   \end{align*}
   #+END_EXPORT
   Here, the probability of the data under the network is given by
   #+BEGIN_EXPORT latex
   \begin{align*}
     P(D | N) &= \prod_{t = 2}^{T} \prod_{i = 1}^m P\left(g_i(t) | G(t-1), N\right) \\
              &= \prod_{t = 2}^{T} \prod_{i = 1}^m \left( p \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(G(t-1)) = g_i(t)} + (1-p) \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(G(t-1)) \neq g_i(t)} \right),
   \end{align*}
   #+END_EXPORT
   where \(G(t-1) = \left\{ g_i(t-1) : i = 1,...,n \right\}\). Here we used that the process generated is a first order Markov chain and that \(G(t)\) is independent given \(G(t-1)\).

   Furthermore, for the prior \(P(N)\) we use an MDL approach as follows.

   Generally, we encode each \(f_i^{(l)}\) independently, i.e. we do not assume that different functions share any input or structure worth exploiting.

   Hence, we assume that \(-\log P(N)\) is of the form
   #+BEGIN_EXPORT latex
   \begin{align*}
   -\log P(N) = \sum_{i, l}^{} L(f_i^{(l)})
   \end{align*}
   #+END_EXPORT
   where \(L(f)\) is an encoding of the function \(f\).


   First, for each function \(f_{i}^{(l)}\) we have to encode how many inputs \(k_{i,l}\) it takes, taking \(L_{\mathbb{N}}(k_{i,l})\) bits, where \(L_{\mathbb{N}}(n) = \log n + \log\log n + \log\log\log n + ... + \log c_0 \) where sum goes only over positive terms, and \(c_0 \approx 2.865\) is such that \(\sum_n^{}2^{-L_{\mathbb{N}}(n)} = 1\). This is a universal code for the integers cite:grunwald2007minimum.

   We also need to encode which genes are inputs to the function. We can do using \(\log \binom{n}{k_{i,l}}\) bits. Last, we need to encode the parameters \(a_i \in \left\{ -1, 1 \right\}\) which requires \(\log 2^{k_{i,l}} = k_{i,l}\) bits.

   The overall prior for \(N\) is therefore
   #+BEGIN_EXPORT latex
   \begin{align*}
   -\log P(N) = \sum_{i = 1}^m \left( \sum_{l = 1}^{m_i} L_{\mathbb{N}}(k_{i,l}) + \log \binom{n}{k_{i,l}} + k_{i,l} \right)
   \end{align*}
   #+END_EXPORT

   In total, we therefore need to minimize the overall loss function
   #+BEGIN_EXPORT latex
   \begin{align*}
     &-\log P(D | N) - \log P(N) \\
     = &\prod_{t = 2}^{T} \prod_{i = 1}^m \left( p \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) = g_i(t)} + (1-p) \sum_{l = 1}^{m_i} c_i^{(l)}1_{f_i^{(l)}(\left\{ g_j(t-1) \right\}) \neq g_i(t)} \right)\\
     &+ \sum_{i = 1}^m \left( \sum_{l = 1}^{m_i} L_{\mathbb{N}}(k_{i,l}) \log \binom{n}{k_{i,l}} + k_{i,l} \right)
   \end{align*}
   #+END_EXPORT
   with respect to \(\left\{ a_{ij}^{(l)}, c_i^{(l)} \right\}\).
** Minimizing the Loss
   It shouldn't be surprising that global optimization of this loss function is not feasible.
   We therefore use a greedy bottom-up approach to find a PBN which explains the data well.

   Further, since the optimizations for every gene are independent, we will focus only on the optimization for \(g = g_1\) across all time steps, and drop all unnecessary indices.

   Starting from the empty net \(N_0\), we start by building a single BN as follows:
   Given distinct \(G_k := g_{j_1},...,g_{j_k}\), \(A_k := a_{j_1},...,a_{j_k}\), find
   #+BEGIN_EXPORT latex
   \begin{align*}
   g_{j_{k+1}}, A_{k+1} := \argmin_{g', A'} \left(  -\log P\left(g | f_{(G_k, g'), A'}\right) + L(f_{(G_k, g'), A'}) \right),
   \end{align*}
   #+END_EXPORT
   where \(f_{G, A}\) is the threshold function with inputs \(G\) and parameters \(A\). The term \(-\log P(g | f_{G, A})\) should be understood as \(-\sum_{t = 2}^T\log P(g(t) | f_{G, A}, G(t-1)) \).

   That is, at every step we add a single gene to the regulatory set, and do a joint optimization over all parameters again.
   This joint optimization has been found to perform much better than picking only a single new \(a_{j_{k+1}}\) while leaving the previous parameters \(A_k\) fixed.

   New genes \(g'\) are added until the loss function \(-\log P(g | f) + L(f)\) doesn't improve any longer by adding more genes to the inputs of \(f\). In practice, limiting the number of potential inputs to \(k = 3, 5\) tends to be a good idea to improve running time without hurting performance too much.

   Once we have found \(f^{(1)},...,f^{(L-1)}\), we try to find a new \(f^{(L)}\) as follows.

   For lack of a better heuristic, start by setting \(c^{(L-1)}, c^{(L)} \leftarrow c^{(L-1)}/2\).
   Given \(c\), \(g^{(L)}_{j_1},...,g^{(L)}_{j_k}\), find \(g^{(L)}_{j_{k+1}}\) and \(A^{(L)}_{k+1}\) as before
   #+BEGIN_EXPORT latex
   \begin{align*}
     &g^{(L)}_{j_{k+1}}, A^{(L)}_{k+1} :=\\
     &\argmin_{g', A'} \left(  -\log P\left(g | \left\{ f^{(l)},  f^{(L)}_{(G^{(L)}_k, g'), A'} \right\}\right) -\log P\left(\left\{ f^{(l)}, f^{(L)}_{(G^{(L)}_k, g'), A'} \right\}\right) \right),
   \end{align*}
   #+END_EXPORT
   i.e. we optimize the inputs and parameters for the new function given all other functions in the same way as we did when there were no other functions. It should be noted that this is not at all similar to finding simply a number of functions which all do a reasonably good job of explaining the temporal evolution by themselves and then using a weighted combination of these to create the PBN. Instead, each new regulatory function has to explain parts of the data different from previous inferred functions to be worth the penalty incurred for putting it in the PBN.

   Then, given all the \(\left\{ G_k^{(l)}, A_k^{(l)} \right\}\), we find
   #+BEGIN_EXPORT latex
   \begin{align*}
   c := \argmin_{c'} \left( - \log P\left(g | \left\{ f^{(i)} \right\}, c'\right) - \log P\left(\left\{ f^{(i)} \right\}, c'\right) \right),
   \end{align*}
   #+END_EXPORT
   i.e. we find the best probabilities given all the other parameters.

   The procedure stops when no new genes are found.

   After all \(f^{(i)}, c^{(i)}\) have been computed, we can prune all functions for which \(c^{(i)} < t\) where \(t\) is an arbitrary threshold. If no pruning is desired, \(t = 0\).

   A high-level overview of the algorithm is given in...
* Results
  All experiments are performed with a cap of \(3\) inferred inputs per regulatory function, and a maximum of \(3\) inferred regulatory functions per gene.

  To evaluate the networks, we compare the inferred network to the true network both in terms of the loss as well as the F-score in finding the correct edges of the true network.

  To compare the loss functions, we generate 50 new time series à 10 points and compute the total loss for both the inferred as well as the true network.

  These losses are shown in \Cref{fig:losses1} for the true network (red) and the inferred network (blue) for sizes 5, 10, 20, 50 and 5 inputs per gene.
  The inferred network never does much (more than a factor 2, say,) worse than the true network. However, it is not clear if something similar can be said for much larger network sizes.

  This is all despite the case that we have put a hard constraint on the number of inputs per function which is lower than the true number.

  #+BEGIN_EXPORT latex
  \begin{figure}[H]
    \centering
    \subfloat{\includegraphics[scale=.40]{R/plots/losses-k-5_e-20_topology-homogeneous.png}}%
    \subfloat{\includegraphics[scale=.40]{R/plots/losses-k-5_e-50_topology-homogeneous.png}}%
    \caption{Losses for true (red) and inferred (blue) networks for \(n=5, 10, 20, 50\) genes using a scalefree topology with exponent 2.5. Left: 20 timeseries à 10 points. Right: 50 timeseries à 10 points.}\\%
    \label{fig:losses1}
    \subfloat{\includegraphics[scale=.40]{R/plots/losses-k-5_e-20_topology-scale_free.png}}%
    \subfloat{\includegraphics[scale=.40]{R/plots/losses-k-5_e-50_topology-scale_free.png}}%
    \caption{Losses for true (red) and inferred (blue) networks for \(n=5, 10, 20, 50\) genes using a scalefree topology with exponent 2.5. Left: 20 timeseries à 10 points. Right: 50 timeseries à 10 points.}
    \label{fig:losses2}
  \end{figure}
  #+END_EXPORT

  Similar, in \Cref{fig:losses2} we find similar results for scale-free networks with in-degrees distributed according to \(d^{-\gamma}\) with \(\gamma = 2.5\).

  #+BEGIN_EXPORT latex
  \begin{figure}[H]
    \centering
    \subfloat{\includegraphics[scale=.40]{R/plots/fscore-homogeneous.png}}%
    \subfloat{\includegraphics[scale=.40]{R/plots/fscore-scale_free.png}}%
    \caption{F-score of the inferred network for sizes \(n=5, 10 , 20, 50\). Left: Homogeneous topology. Right: Scalefree topology.}\\%
    \label{fig:f1}
  \end{figure}
  #+END_EXPORT

  Note that while both true and inferred networks do worse here, the inferred network doesn't do any worse relative to the true network despite the fact that our constraint on using only few inputs should hurt us even more in this case.

  In \Cref{fig:f1} the F-scores for networks of different size and using different number of experiments are shown.

  Further, in \Cref{fig:times}, we see that for homogeneous topology on the network, the runtime scales roughly quadratically with the size of the networks. This is exactly what we would have expected simply from the description of the algorithm above.

  Further comparing the two plots it appears like the time taken for the inference is roughly linear in the number of parallel time series we are running. This also isn't surprising.

  As the amount of time taken for networks with scalefree topology is virtually indistinguishable from those shown in \Cref{fig:times}, they are not shown additionally.

  #+BEGIN_EXPORT latex
  \begin{figure}[t]
    \centering
    \subfloat{\includegraphics[scale=.40]{R/plots/times-k-5_e-20_topology-homogeneous.png}}%
    \subfloat{\includegraphics[scale=.40]{R/plots/times-k-5_e-50_topology-homogeneous.png}}\\%
    \caption{Times for network inference in seconds for \(n=5, 10, 20, 50\) genes using homogeneous topology. Left: 20 timeseries à 10 points. Right: 50 timeseries à 10 points.}
    \label{fig:times}
  \end{figure}
  #+END_EXPORT
* Further Ideas
  From a coding perspective, instead of computing \(P(D | N)\) by summing over probabilities of networks predicting the correct outcome, one could also use \(H\left(\left\{ c_j^{(l)} : l = 1,...,L \right\}\right)\) bits to encode which network is the "correct one" at every time step and try to find that model which requires the least number of bits to encode the outcome.

  Furthermore, since the network inference process quickly becomes expensive the more data is used, it might be a good idea to use subsampling methods.

  Another way to reduce the computational load here would be to consider only those genes which show a high level of (lagged) mutual information with the target genes.
* Related research
  Full Bayesian Inference for BNs have been made use of only quite recently cite:han2014full.
  Previously, the MDL principle has been used for inferring gene regulatory networks in cite:tabus2001use,tabus2003normalized,zhao2006inferring,dougherty2008inference,chaitankar2009gene,chaitankar2010novel. However, these approaches were always applied to finding a single model like a BNp rather than a combination of BNs as in a PBN.
  Previous attempts at inferring PBNs from timeseries data were cite:marshall2006temporal,marshall2007inference, trying to make use of context switches between different PBNs and inferring one BN for every single such context. However, they required timeseries consisting of hundreds of contiguous measurements and were found to have problems finding context switches even then.
  Another approach by cite:trairatphisan2014optpbn uses logic optimization, with a naive 0-1-loss.
  \printbibliography
